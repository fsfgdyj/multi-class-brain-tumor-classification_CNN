This README outlines the motivation, objectives, and methodology for developing a deep learning model for brain tumor classification from MRI scans. The techniques employed address the challenges of imbalanced datasets, ensuring a fair and accurate model. For detailed implementation steps, please refer to the documentation.Feel free to contribute, share insights, or collaborate to enhance the accuracy and versatility of the model. Together, we can make significant strides in medical image analysis and computer vision.Deep learning-based brain tumor classification from brain magnetic resonance imaging (MRI) is asignificant research problem. The research problem encounters a major challenge. The training datasets used to develop deep learning algorithms could be imbalanced with significantly more samples for one type of tumor than others. This imbalance in the training dataset affects the performance of tumor classification using deep learning models as the classifier performance gets biased towards the majority class.The objective of this study is to develop a model for detecting and classifying objects in images.The accurate identification of objects in images has various applications in computer vision and object recognition tasks. This methodology outlines the steps taken to build and train aconvolutional neural network (CNN) model for this purpose.
![web1](https://github.com/jayd-bit/multi-class-brain-tumor-classification_CNN/assets/132098211/5f560076-b272-4f80-9768-032895bf1674)
![web2](https://github.com/jayd-bit/multi-class-brain-tumor-classification_CNN/assets/132098211/779f6476-ef39-4a1d-8aff-5a1bcd7981a4)
![web3](https://github.com/jayd-bit/multi-class-brain-tumor-classification_CNN/assets/132098211/79f4eccd-be36-4b76-ae07-8335b89609c6)
![web4](https://github.com/jayd-bit/multi-class-brain-tumor-classification_CNN/assets/132098211/38236121-956c-4116-854b-fb71f9b5068a)
![loss_metrics](https://github.com/jayd-bit/multi-class-brain-tumor-classification_CNN/assets/132098211/cf453ded-0a60-43a9-839d-d3558e569e49)
